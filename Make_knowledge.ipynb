{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy #https://github.com/tweepy/tweepy\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up API and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twitter API credentials\n",
    "consumer_key = \"CONSUMER_KEY\"\n",
    "consumer_secret = \"CONSUMER_SECRET\"\n",
    "access_key = \"ACCESS_KEY\"\n",
    "access_secret = \"MAKE_AND_MODEL_OF_FIRST_CAR\"\n",
    "\n",
    "\n",
    "\n",
    "OAUTH_KEYS = {'consumer_key':consumer_key, 'consumer_secret':consumer_secret,\n",
    " 'access_token_key':access_key, 'access_token_secret':access_secret}\n",
    "auth = tweepy.OAuthHandler(OAUTH_KEYS['consumer_key'], OAUTH_KEYS['consumer_secret'])\n",
    "\n",
    "# In order to manage the rate limiting, use these options below. \n",
    "# You will find later that rate limiting is, well, a limiting factor.\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what we want to do here is generate a corpus of text, from which the markov model can generate probabilities. The steps look like this:\n",
    "\n",
    "1. Collect the data\n",
    "2. 'Clean' the data\n",
    "3. Combine the data\n",
    "4. Apply Markov model\n",
    "5. Generate some example tweets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I want to collect tweets from all the 'friends' of the account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing thought leadership from friends of infosec_truths\n",
      "importing thought leadership from derekcoulson\n",
      "importing thought leadership from ByrneGh\n",
      "importing thought leadership from BartInglot\n",
      "importing thought leadership from 3dRailForensics\n",
      "importing thought leadership from TunnelsUp\n",
      "importing thought leadership from spresec\n",
      "importing thought leadership from mttcrns\n",
      "importing thought leadership from BarryV\n",
      "importing thought leadership from ISecPlayasClub\n",
      "importing thought leadership from MikeOppenheim\n",
      "importing thought leadership from jepayneMSFT\n",
      "importing thought leadership from gentilkiwi\n",
      "importing thought leadership from harmj0y\n",
      "importing thought leadership from markrussinovich\n",
      "importing thought leadership from vysecurity\n",
      "importing thought leadership from PyroTek3\n",
      "importing thought leadership from jaredhaight\n",
      "importing thought leadership from robknake\n",
      "importing thought leadership from RobertMLee\n",
      "importing thought leadership from cnoanalysis\n",
      "importing thought leadership from JohnHultquist\n",
      "importing thought leadership from cyb3rops\n",
      "importing thought leadership from Hexacorn\n",
      "importing thought leadership from pwnallthethings\n",
      "importing thought leadership from malwareunicorn\n",
      "importing thought leadership from jackcr\n",
      "importing thought leadership from Cyb3rWard0g\n",
      "importing thought leadership from _devonkerr_\n",
      "importing thought leadership from MITREattack\n",
      "importing thought leadership from enigma0x3\n",
      "importing thought leadership from mattifestation\n",
      "importing thought leadership from MalwareTechBlog\n",
      "importing thought leadership from thegrugq\n",
      "importing thought leadership from SwiftOnSecurity\n",
      "importing thought leadership from kwm\n",
      "importing thought leadership from subTee\n",
      "importing thought leadership from christruncer\n",
      "importing thought leadership from QW5kcmV3\n",
      "importing thought leadership from lucaskossack\n",
      "importing thought leadership from Malwarenailed\n",
      "importing thought leadership from Glasswalk3r\n",
      "importing thought leadership from Matt_Grandy_\n",
      "importing thought leadership from deantyler\n",
      "importing thought leadership from williballenthin\n",
      "importing thought leadership from secbern\n",
      "importing thought leadership from generationlext\n",
      "importing thought leadership from cglyer\n",
      "importing thought leadership from matthewdunwoody\n",
      "importing thought leadership from bwithnell\n",
      "importing thought leadership from stvemillertime\n",
      "importing thought leadership from danielhbohannon\n",
      "importing thought leadership from ItsReallyNick\n"
     ]
    }
   ],
   "source": [
    "\n",
    "THECORPUSOFMYMIND = []\n",
    "thought_leaders = []\n",
    "user = 'infosec_truths'\n",
    "print(\"importing thought leadership from friends of {}\".format(user))\n",
    "for friend in api.friends_ids(user):\n",
    "    sn = api.get_user(friend).screen_name\n",
    "    thought_leaders.append(sn)\n",
    "\n",
    "    \n",
    "\n",
    "for thought_leader in thought_leaders:\n",
    "    #print(\"importing thought leadership from {}\".format(thought_leader))\n",
    "    tweets = api.user_timeline(screen_name =thought_leader,count=1000)\n",
    "    for tweet in tweets:\n",
    "            if not tweet.retweeted:\n",
    "                THECORPUSOFMYMIND.append(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I wanted to remove not-thought-leading tweets, like Re-tweets, @ mentions, and stuff with useful links in them. I want just the pure, text-only, stream-of-consciousness thought leadership that makes infosec twitter great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    for tweet in THECORPUSOFMYMIND:\n",
    "        if \"@\" in tweet:\n",
    "            THECORPUSOFMYMIND.remove(tweet)\n",
    "    \n",
    "http_regex = r\"http.+?(?=[ \\\"\\']|$|\\n)\"\n",
    "at_regex = r\"@.+?(?= |$)\"\n",
    "RT_regex = r\"^RT.+?(?=[^ ])\"\n",
    "\n",
    "\n",
    "for tweet in THECORPUSOFMYMIND:\n",
    "    idx = THECORPUSOFMYMIND.index(tweet)\n",
    "    tweet = re.sub(http_regex, '', tweet)\n",
    "    tweet = re.sub(at_regex, '', tweet)\n",
    "    tweet = re.sub(RT_regex, '', tweet)\n",
    "    THECORPUSOFMYMIND[idx] = tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweetlist):\n",
    "    for tweet in tweetlist:\n",
    "        idx = tweetlist.index(tweet)\n",
    "        tweet = re.sub(http_regex, '', tweet)\n",
    "        tweet = re.sub(at_regex, '', tweet)\n",
    "        tweet = re.sub(RT_regex, '', tweet)\n",
    "        tweetlist[idx] = tweet\n",
    "    return tweetlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a sneak peek just to make sure we're getting what we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' So many small security software companies use it...',\n",
       " '\"Our product will fix the problem that is thought to have caused X beach\"  is a misleading sales pitch, and also re… ',\n",
       " 'You should not be doing anything sexy until you can tell me that AV is installed on each endpoint and have mechanis… ',\n",
       " 'somewhere in China a forensic analyst just found out that he wasted days and nights analyzing old systems ',\n",
       " '  True, having the data to search through is a great thing.',\n",
       " '#braggingrights ',\n",
       " 'Once a transient city, over the last few years people have fallen in love with it -  buying homes and investing in… ',\n",
       " ' Later they would find a way to get our military technology without putting lives at risk...',\n",
       " 'I want to know the collective time wasted by a single milkshake order at Potbelly during the lunch rush.',\n",
       " 'This is going to exacerbate my issues with session hoarding. ']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "THECORPUSOFMYMIND[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I join the corpus into a single document I can use to generate the markov model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINALCORPUS = \" \".join(THECORPUSOFMYMIND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINALCORPUS = FINALCORPUS.replace('\\r', '').replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASCIICORPUS = str(FINALCORPUS.encode('ascii',errors='ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\\' So many small security software companies use it... \"Our product will fix the problem that is thought to have caused X beach\"  is a misleading sales pitch, and also re  You should not be doing anything sexy until you can tell me that AV is installed on each endpoint and have mechanis  somewhere in China a forensic analyst just found out that he wasted days and nights analyzing old systems    True, having the data to search through is a great thing. #braggingrights  Once a transient city, over the last few years people have fallen in love with it -  buying homes and investing in   Later they would find a way to get our military technology without putting lives at risk... I want to know the collective time wasted by a single milkshake order at Potbelly during the lunch rush. This is going to exacerbate my issues with session hoarding.  And couldn\\\\\\'t manage to steal a better metal than bronze  Couple is watching videos on phone without headphones. Flight attendant hands them free earbu'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ASCIICORPUS[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add to existing corpus document\n",
    "text_file = open(\"corpus.txt\", \"w\")\n",
    "text_file.write(ASCIICORPUS)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Markov model, I use 'markovify', which I found from this @ChrisAlbon page: https://chrisalbon.com/python/other/generate_tweets_using_markov_chain/ \n",
    "\n",
    "Sidenote: if you don't use Chris Albon pages to learn python/data science, you should.\n",
    "\n",
    "Now the fun part! Let's thought lead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to pull together a list of awesome people to follow on twitter, there are so many or bec This time with the FBI that they have standup desks!\n",
      "\n",
      "Look at the sample yet, but here\\'s a more complete list:0: Spiders Outcomes are what matter.\n",
      "\n",
      "Nice detail about the challenges and their host &amp; network artifacts and behaviors!\n",
      "\n",
      "If you missed my SANS webcast on key Sysinternals tools for data analysis are on LinkedIn, so I read body language very very well...\n",
      "\n",
      "The Enterprise ATT&amp;CK site has been missing a page on the best security on the tips Tips for searching for rogue processes:It started with the countless sacrifices of others you better have a narrow aperture, it can lead you to believe something is incredibly rare.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import markovify\n",
    "\n",
    "# Get raw text as string.\n",
    "with open(\"corpus.txt\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Build the model.\n",
    "text_model = markovify.Text(text)\n",
    "\n",
    "# Print five randomly-generated sentences\n",
    "\n",
    "for i in range(5):\n",
    "    print(text_model.make_short_sentence(280))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps:\n",
    "\n",
    "As you can see, the markov approach _kind of_ works...but makes a lot of nonsense. That's partly because the markov model only cares about the next word, not the whole sentence or thought. To build a model more aware of the structure of language, other folks have used neural networks, perhaps the most hilarious/ infamous being @deepdrumpf: https://twitter.com/deepdrumpf\n",
    "\n",
    "News here: \n",
    "\n",
    "https://www.forbes.com/sites/janetwburns/2016/10/19/deepdrumpf-is-an-uncanny-twitterbot-thats-fundraising-for-girls-in-stem/#1f078fa649da\n",
    "\n",
    "https://www.theguardian.com/technology/2016/mar/04/donald-trump-deep-drumpf-twitter-bot\n",
    "\n",
    "This seems like the logical next step for infosec_truths. There are a bunch of tutorials online as well:\n",
    "\n",
    "https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "https://dzone.com/articles/generating-tweets-using-a-recurrent-neural-net-tor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
