{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy #https://github.com/tweepy/tweepy\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up API and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twitter API credentials\n",
    "consumer_key = \"CONSUMER_KEY\"\n",
    "consumer_secret = \"CONSUMER_SECRET\"\n",
    "access_key = \"ACCESS_KEY\"\n",
    "access_secret = \"MOTHERS_MAIDEN_NAME\"\n",
    "\n",
    "\n",
    "\n",
    "OAUTH_KEYS = {'consumer_key':consumer_key, 'consumer_secret':consumer_secret,\n",
    " 'access_token_key':access_key, 'access_token_secret':access_secret}\n",
    "auth = tweepy.OAuthHandler(OAUTH_KEYS['consumer_key'], OAUTH_KEYS['consumer_secret'])\n",
    "\n",
    "# In order to manage the rate limiting, use these options below. \n",
    "# You will find later that rate limiting is, well, a limiting factor.\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what we want to do here is generate a corpus of text, from which the markov model can generate probabilities. The steps look like this:\n",
    "\n",
    "1. Collect the data\n",
    "2. 'Clean' the data\n",
    "3. Combine the data\n",
    "4. Apply Markov model\n",
    "5. Generate some example tweets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I want to collect tweets from all the 'friends' of the account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing thought leadership from friends of infosec_truths\n"
     ]
    }
   ],
   "source": [
    "\n",
    "THECORPUSOFMYMIND = []\n",
    "thought_leaders = []\n",
    "user = 'infosec_truths'\n",
    "print(\"importing thought leadership from friends of {}\".format(user))\n",
    "for friend in api.friends_ids(user):\n",
    "    sn = api.get_user(friend).screen_name\n",
    "    thought_leaders.append(sn)\n",
    "\n",
    "    \n",
    "\n",
    "for thought_leader in thought_leaders:\n",
    "    #print(\"importing thought leadership from {}\".format(thought_leader))\n",
    "    tweets = api.user_timeline(screen_name =thought_leader,count=1000)\n",
    "    for tweet in tweets:\n",
    "            if not tweet.retweeted:\n",
    "                THECORPUSOFMYMIND.append(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I wanted to remove not-thought-leading tweets, like Re-tweets, @ mentions, and stuff with useful links in them. I want just the pure, text-only, stream-of-consciousness thought leadership that makes infosec twitter great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    for tweet in THECORPUSOFMYMIND:\n",
    "        if \"@\" in tweet:\n",
    "            THECORPUSOFMYMIND.remove(tweet)\n",
    "    \n",
    "http_regex = r\"http.+?(?=[ \\\"\\']|$|\\n)\"\n",
    "at_regex = r\"@.+?(?= |$)\"\n",
    "RT_regex = r\"^RT.+?(?=[^ ])\"\n",
    "\n",
    "\n",
    "for tweet in THECORPUSOFMYMIND:\n",
    "    idx = THECORPUSOFMYMIND.index(tweet)\n",
    "    tweet = re.sub(http_regex, '', tweet)\n",
    "    tweet = re.sub(at_regex, '', tweet)\n",
    "    tweet = re.sub(RT_regex, '', tweet)\n",
    "    THECORPUSOFMYMIND[idx] = tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweetlist):\n",
    "    for tweet in tweetlist:\n",
    "        idx = tweetlist.index(tweet)\n",
    "        tweet = re.sub(http_regex, '', tweet)\n",
    "        tweet = re.sub(at_regex, '', tweet)\n",
    "        tweet = re.sub(RT_regex, '', tweet)\n",
    "        tweetlist[idx] = tweet\n",
    "    return tweetlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a sneak peek just to make sure we're getting what we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Important point made: Your personal email account is the key to everything else. ',\n",
       " '\"Our product will fix the problem that is thought to have caused X beach\"  is a misleading sales pitch, and also re… ',\n",
       " 'You should not be doing anything sexy until you can tell me that AV is installed on each endpoint and have mechanis… ',\n",
       " 'somewhere in China a forensic analyst just found out that he wasted days and nights analyzing old systems ',\n",
       " '  True, having the data to search through is a great thing.',\n",
       " '#braggingrights ',\n",
       " 'Once a transient city, over the last few years people have fallen in love with it -  buying homes and investing in… ',\n",
       " ' Later they would find a way to get our military technology without putting lives at risk...',\n",
       " 'I want to know the collective time wasted by a single milkshake order at Potbelly during the lunch rush.',\n",
       " 'This is going to exacerbate my issues with session hoarding. ']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "THECORPUSOFMYMIND[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I join the corpus into a single document I can use to generate the markov model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINALCORPUS = \" \".join(THECORPUSOFMYMIND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINALCORPUS = FINALCORPUS.replace('\\r', '').replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASCIICORPUS = str(FINALCORPUS.encode('ascii',errors='ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\\'Important point made: Your personal email account is the key to everything else.  \"Our product will fix the problem that is thought to have caused X beach\"  is a misleading sales pitch, and also re  You should not be doing anything sexy until you can tell me that AV is installed on each endpoint and have mechanis  somewhere in China a forensic analyst just found out that he wasted days and nights analyzing old systems    True, having the data to search through is a great thing. #braggingrights  Once a transient city, over the last few years people have fallen in love with it -  buying homes and investing in   Later they would find a way to get our military technology without putting lives at risk... I want to know the collective time wasted by a single milkshake order at Potbelly during the lunch rush. This is going to exacerbate my issues with session hoarding.  And couldn\\\\\\'t manage to steal a better metal than bronze  Couple is watching videos on phone without headphones. Flight at'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ASCIICORPUS[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add to existing corpus document\n",
    "text_file = open(\"corpus.txt\", \"w\")\n",
    "text_file.write(ASCIICORPUS)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Markov model, I use 'markovify', which I found from this @ChrisAlbon page: https://chrisalbon.com/python/other/generate_tweets_using_markov_chain/ \n",
    "\n",
    "Sidenote: if you don't use Chris Albon pages to learn python/data science, you should.\n",
    "\n",
    "Now the fun part! Let's thought lead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sound only coming out of prison to find evil, always apply them against older datasets.\n",
      "\n",
      "There should be horrified Dont make important mathematical decisions for your story of how If you\\'re in US jurisdiction.\n",
      "\n",
      "Defcon is for everyone, but also praise good web UI... and hmm this is called PowerShell Direct and it\\'s what allows you to alert on critical events, search as needed.\n",
      "\n",
      "Unnamed pass through most likely used by Windows, boy does this page have you covered: CSS is the most promising trends in information security is the 5th floor for #DayofShecurity.\n",
      "\n",
      "A pipeline for telemetry that allows you to check that i was not a security vendor #infosec #Uber I wonder if hackers laugh or enjoy how we refer to them...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import markovify\n",
    "\n",
    "# Get raw text as string.\n",
    "with open(\"corpus.txt\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Build the model.\n",
    "text_model = markovify.Text(text)\n",
    "\n",
    "# Print five randomly-generated sentences\n",
    "\n",
    "for i in range(5):\n",
    "    print(text_model.make_short_sentence(280))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps:\n",
    "\n",
    "As you can see, the markov approach _kind of_ works...but makes a lot of nonsense. That's partly because the markov model only cares about the next word, not the whole sentence or thought. To build a model more aware of the structure of language, other folks have used neural networks, perhaps the most hilarious/ infamous being @deepdrumpf: https://twitter.com/deepdrumpf\n",
    "\n",
    "News here: \n",
    "\n",
    "https://www.forbes.com/sites/janetwburns/2016/10/19/deepdrumpf-is-an-uncanny-twitterbot-thats-fundraising-for-girls-in-stem/#1f078fa649da\n",
    "\n",
    "https://www.theguardian.com/technology/2016/mar/04/donald-trump-deep-drumpf-twitter-bot\n",
    "\n",
    "This seems like the logical next step for infosec_truths. There are a bunch of tutorials online as well:\n",
    "\n",
    "https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "https://dzone.com/articles/generating-tweets-using-a-recurrent-neural-net-tor\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
